{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "subsequent-macedonia",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "wrapped-artist",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 1, 4, 100, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "later-content",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random Tensors to hold inputs and outputs\n",
    "z = torch.randn(N, D_in, device=device)\n",
    "y_hat = torch.randn(N, D_out, device=device)\n",
    "target=torch.tensor([1.0]) ### label of the current example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "connected-lloyd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(\n",
    "          torch.nn.Linear(D_in, H),\n",
    "          torch.nn.ReLU(),\n",
    "          torch.nn.Linear(H, D_out),\n",
    "          torch.nn.Sigmoid(), ## the output layer\n",
    "        ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "professional-input",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.BCELoss() ## remember - it is simply the minus of log likelihood - training via Max likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "waiting-dairy",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat=model(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bored-paraguay",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5449]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bored-debut",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda2/envs/ICML2021/lib/python3.7/site-packages/torch/nn/modules/loss.py:529: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "loss=loss_fn(y_hat, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "automatic-kuwait",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.zero_grad()\n",
    "loss.backward() ## computes the gradient of the loss w.r.t. the NN params includes bias terms as well "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "historic-heather",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_vec=[]\n",
    "for param in model.parameters():\n",
    "    grad_vec.append(param.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "integral-analyst",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_vec[1].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "social-fitness",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure: Sequential(\n",
      "  (0): Linear(in_features=4, out_features=100, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=100, out_features=1, bias=True)\n",
      "  (3): Sigmoid()\n",
      ")\n",
      "\n",
      "\n",
      "Layer: 0.weight | Size: torch.Size([100, 4]) | Values : tensor([[-0.4595, -0.0075, -0.4195, -0.0086],\n",
      "        [ 0.1772,  0.3407,  0.3167, -0.4493]], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: 0.bias | Size: torch.Size([100]) | Values : tensor([-0.2647, -0.4026], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: 2.weight | Size: torch.Size([1, 100]) | Values : tensor([[ 0.0527, -0.0407,  0.0136,  0.0572, -0.0079,  0.0133, -0.0621, -0.0869,\n",
      "          0.0482,  0.0178, -0.0057, -0.0292, -0.0480, -0.0279,  0.0773,  0.0536,\n",
      "          0.0660,  0.0047, -0.0896,  0.0308,  0.0377,  0.0981, -0.0340,  0.0972,\n",
      "         -0.0991,  0.0966,  0.0535,  0.0211,  0.0389,  0.0437,  0.0021, -0.0193,\n",
      "          0.0786,  0.0460, -0.0057, -0.0051, -0.0475, -0.0508,  0.0852,  0.0675,\n",
      "         -0.0190,  0.0197, -0.0555,  0.0556, -0.0989,  0.0411,  0.0930, -0.0831,\n",
      "          0.0485,  0.0710, -0.0642,  0.0503,  0.0954, -0.0521, -0.0405,  0.0309,\n",
      "         -0.0803, -0.0416, -0.0133,  0.0838,  0.0848, -0.0253,  0.0831,  0.0849,\n",
      "          0.0564,  0.0153,  0.0251, -0.0414, -0.0812,  0.0930,  0.0582, -0.0840,\n",
      "          0.0760, -0.0267, -0.0730, -0.0261, -0.0637, -0.0596,  0.0925,  0.0159,\n",
      "          0.0028, -0.0563,  0.0467,  0.0642, -0.0949,  0.0786,  0.0744, -0.0164,\n",
      "         -0.0852,  0.0448,  0.0959,  0.0229, -0.0563, -0.0204, -0.0986, -0.0105,\n",
      "         -0.0813,  0.0710,  0.0844, -0.0827]], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: 2.bias | Size: torch.Size([1]) | Values : tensor([-0.0053], grad_fn=<SliceBackward>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model structure: {model}\\n\\n\")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "molecular-bronze",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
